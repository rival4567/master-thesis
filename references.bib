@article{russmann2015industry,
  title={Industry 4.0: The future of productivity and growth in manufacturing industries},
  author={R{\"u}{\ss}mann, Michael and Lorenz, Markus and Gerbert, Philipp and Waldner, Manuela and Justus, Jan and Engel, Pascal and Harnisch, Michael},
  year={2015}
}

@manual{visor_user_manual,
  author       = {SENSOPART},
  title        = {VISOR\textsuperscript{\textregistered} User Manual Software Version 2.8},
  year         = {2024},
}

@article{SEGURA2021100060,
title = {Human-robot collaborative systems: Structural components for current manufacturing applications},
journal = {Advances in Industrial and Manufacturing Engineering},
volume = {3},
pages = {100060},
year = {2021},
issn = {2666-9129},
doi = {https://doi.org/10.1016/j.aime.2021.100060},
url = {https://www.sciencedirect.com/science/article/pii/S2666912921000301},
author = {Pablo Segura and Odette Lobato-Calleros and Alejandro Ramírez-Serrano and Isidro Soria},
keywords = {Human-robot collaborative systems, Collaborative robots, Assembly robots, Computer-integrated manufacturing, Industry 4.0},
abstract = {The implementation of human-robot collaborative systems in industrial environments have widely extended during the last five years, from manufacturing applications reproduced in laboratory facilities or digital simulations to real automotive shop floors. Commonly, one way to guide their design has been through the adoption of international standards focused solely on the safe operation of collaborative robots. The main objective of this paper is the identification of basic components comprising human-robot collaborative systems design. This is supported by two steps, 1) Provide an extensive compendium of current applications and components within a varied set of manufacturing sectors and tasks. 2) Based on the latter, propose a selection of “structural components” for collaborative work. We conceptualized structural components as the organizational and technological alternatives necessary to fulfil the basic requirements and functionalities of human-robot collaborative systems. This document presents a systematic literature review that includes 50 exemplary case studies implemented in different manufacturing environments throughout the last five years praxis (2016–2020). Four structural components were identified in this paper: interaction levels, work roles, communication interfaces and safety control modes. Furthermore, it was found that physical contact-based collaboration for screwing assembly of small-sized parts and material handling of heavyweight objects are suitable applications for the automotive industry. Moreover, certified augmented and virtual reality devices were highlighted as convenient assistive technologies for safety and training manufacturing needs. The presented categorization will allow practitioners on selecting settings of compatible structural components that could respond better to trendy manufacturing requirements searching for highly personalized products.}
}

@online{kassowrobotsblog,
  author = {Steger, Adam},
  title = {What is Robotic Process Automation(RPA)? The Complete 2024 Guide},
  year = {2024},
  url = {https://www.kassowrobots.com/blog/what-is-robotic-process-automation-rpa},
  note = {Accessed: 2024-08-07}
}

@article{BAI2020107776,
title = {Industry 4.0 technologies assessment: A sustainability perspective},
journal = {International Journal of Production Economics},
volume = {229},
pages = {107776},
year = {2020},
issn = {0925-5273},
doi = {https://doi.org/10.1016/j.ijpe.2020.107776},
url = {https://www.sciencedirect.com/science/article/pii/S0925527320301559},
author = {Chunguang Bai and Patrick Dallasega and Guido Orzes and Joseph Sarkis},
keywords = {Industry 4.0, Technology, Sustainability, Hesitant fuzzy set, Cumulative prospect theory, VIKOR},
abstract = {The fourth industrial revolution, also labelled Industry 4.0, was beget with emergent and disruptive intelligence and information technologies. These new technologies are enabling ever-higher levels of production efficiencies. They also have the potential to dramatically influence social and environmental sustainable development. Organizations need to consider Industry 4.0 technologies contribution to sustainability. Sufficient guidance, in this respect, is lacking in the scholarly or practitioner literature. In this study, we further examine Industry 4.0 technologies in terms of application and sustainability implications. We introduce a measures framework for sustainability based on the United Nations Sustainable Development Goals; incorporating various economic, environmental and social attributes. We also develop a hybrid multi-situation decision method integrating hesitant fuzzy set, cumulative prospect theory and VIKOR. This method can effectively evaluate Industry 4.0 technologies based on their sustainable performance and application. We apply the method using secondary case information from a report of the World Economic Forum. The results show that mobile technology has the greatest impact on sustainability in all industries, and nanotechnology, mobile technology, simulation and drones have the highest impact on sustainability in the automotive, electronics, food and beverage, and textile, apparel and footwear industries, respectively. Our recommendation is to take advantage of Industry 4.0 technology adoption to improve sustainability impact but each technology needs to be carefully evaluated as specific technology will variably influence industry and sustainability dimensions. Investment in such technologies should consider appropriate priority investment and championing.}
}


@INPROCEEDINGS{8361333,
  author={Yang, Sida and Xu, Wenjun and Liu, Zhihao and Zhou, Zude and Pham, Duc Truong},
  booktitle={2018 IEEE 15th International Conference on Networking, Sensing and Control (ICNSC)}, 
  title={Multi-source vision perception for human-robot collaboration in manufacturing}, 
  year={2018},
  volume={},
  number={},
  pages={1-6},
  keywords={Collaboration;Three-dimensional displays;Robot sensing systems;Cameras;Service robots;Manufacturing;human-robot collaboration;manufacturing;multi-source vision perception;data fusion},
  doi={10.1109/ICNSC.2018.8361333}
}

@article{CHEN2001199,
title = {Rapid response manufacturing through a rapidly reconfigurable robotic workcell},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {17},
number = {3},
pages = {199-213},
year = {2001},
issn = {0736-5845},
doi = {https://doi.org/10.1016/S0736-5845(00)00028-4},
url = {https://www.sciencedirect.com/science/article/pii/S0736584500000284},
author = {I-Ming Chen},
keywords = {Modular robot, Component technology, Agile manufacturing, Reconfigurable automation},
abstract = {This article describes the development of a component-based technology robot workcell that can be rapidly configured to perform a specific manufacturing task. The workcell is conceived with standard and inter-operable components including actuator modules, rigid link connectors and tools that can be assembled into robots with arbitrary geometry and degrees of freedom. The reconfigurable “plug-and-play” robot kinematic and dynamic modeling algorithms are developed. These algorithms are the basis for the control and simulation of reconfigurable robots. The concept of robot configuration optimization is introduced for the effective use of the rapidly reconfigurable robots. Control and communications of the workcell components are facilitated by a workcell-wide TCP/IP network and device-level CAN-bus networks. An object-oriented simulation and visualization software for the reconfigurable robot is developed based on Windows NT. Prototypes of the robot workcells configured to perform the light-machining task and the positioning task are constructed and demonstrated.}
}

@article{WAGNER201988,
title = {Challenges and Potentials of Digital Twins and Industry 4.0 in Product Design and Production for High Performance Products},
journal = {Procedia CIRP},
volume = {84},
pages = {88-93},
year = {2019},
note = {29th CIRP Design Conference 2019, 08-10 May 2019, Póvoa de Varzim, Portgal},
issn = {2212-8271},
doi = {https://doi.org/10.1016/j.procir.2019.04.219},
url = {https://www.sciencedirect.com/science/article/pii/S2212827119308637},
author = {Raphael Wagner and Benjamin Schleich and Benjamin Haefner and Andreas Kuhnle and Sandro Wartzack and Gisela Lanza},
keywords = {Product development, Production planning, Information},
abstract = {Digital twins offer great opportunities in various domains of the product engineering process. However, current approaches to the use of digital twins only focus on different separated disciplines. In contrast to that, it is expected that the holistic use of digital twin models in product development and production will dominate future product generations, because they allow to create high-performance products competitively. This paper explores important challenges and future potentials of digital twins and Industry 4.0 for the seamless integration of product specification and production. In this regard, approaches of linking digital twins to other domains open up new possibilities in tolerance allocation and production integration. Thereby, the most efficient product specifications in technical and economic terms are achieved for the manufacturer. In addition, the connectivity of Industry 4.0 broadens the scope and enables the evaluation of alternative approaches in production planning and control. Approaches at the organizational level, product functions with specifications beyond the technological limits and production control strategies (e.g. order dispatching) ensure high performance operations. Simulations with a digital production twin with integrated digital product twin allow early estimations even before the actual ramp-up of the product. The future challenge addressed in this paper is to define a consistent framework for the holistic use of digital twins in the entire product development process, which requires the integration of product designers and production planner concepts.}
}

@article{SEMERARO2021103469,
title = {Digital twin paradigm: A systematic literature review},
journal = {Computers in Industry},
volume = {130},
pages = {103469},
year = {2021},
issn = {0166-3615},
doi = {https://doi.org/10.1016/j.compind.2021.103469},
url = {https://www.sciencedirect.com/science/article/pii/S0166361521000762},
author = {Concetta Semeraro and Mario Lezoche and Hervé Panetto and Michele Dassisti},
keywords = {Digital twin, Industry 4.0, Cyber-physical systems, Predictive manufacturing},
abstract = {Manufacturing enterprises are facing the need to align themselves to the new information technologies (IT) and respond to the new challenges of variable market demand. One of the key enablers of this IT revolution toward Smart Manufacturing is the digital twin (DT). It embeds a “virtual” image of the reality constantly synchronized with the real operating scenario to provide sound information (knowledge model) to reality interpretation model to draw sound decisions. The paper aims at providing an up-to date picture of the main DT components, their features and interaction problems. The paper aims at clearly tracing the ongoing research and technical challenges in conceiving and building DTs as well, according to different application domains and related technologies. To this purpose, the main questions answered here are: ‘What is a Digital Twin?’; ‘Where is appropriate to use a Digital Twin?’; ‘When has a Digital Twin to be developed?’; ‘Why should a Digital Twin be used?’; ‘How to design and implement a Digital Twin?’; ‘What are the main challenges of implementing a Digital Twin?’. This study tries to answer to the previous questions funding on a wide systematic literature review of scientific research, tools, and technicalities in different application domains.}
}

@online{digitaltwinblog,
  author = {Industry and Trends},
  title = {Digital Twins and Virtual Commissioning in the Manufacturing Industry (Updated for 2023)},
  year = {2022},
  url = {https://www.visualcomponents.com/blog/digital-twins-and-virtual-commissioning-in-industry-4-0/},
  note = {Accessed: 2024-07-12}
}

@article{antonremote,
  title={REMOTE CONTROL OF A ROBOTIZED FLEXIBLE WORKCELL USING A WEB INTERFACE},
  author={Anton, Florin Daniel and Borangiu, Theodor and Anton, Silvia and Condrut, Andra}
}

@article{Xiao_2019,
doi = {10.1088/1755-1315/252/4/042112},
url = {https://dx.doi.org/10.1088/1755-1315/252/4/042112},
year = {2019},
month = {apr},
publisher = {IOP Publishing},
volume = {252},
number = {4},
pages = {042112},
author = {Zixuan Xiao and Yulin Xu},
title = {Web-Based Robot Control Interface},
journal = {IOP Conference Series: Earth and Environmental Science},
abstract = {A web-based robot control system interface is designed. This interface can be used by users to control the motion of target robot, observe 3D model, and monitor the real-time value of the robot joints through PC or mobile device. The advantage of this system lies in its versatility and portability, allowing users to open the interface to control target robot in any place. This system mainly uses robot operating system (ROS), Html 5, C++, JavaScript and PHP technology. JavaScript library provided by ROS authority is used to build the front-end interface. PHP is used to create a user registration login system. Ros3djs is used to realize the establishment of dynamic model of robot simulation. The C++ library provided by ROS is used to synchronize the communication between the ROS node and the robot. Finally, the human-computer interaction system of the 16-DOF robotic hand is successfully implemented. According to the results, the system has good interactivity, versatility and portability.}
}

@INPROCEEDINGS{10070046,
  author={Islam, Md. Touhidul and Hameem, Imtiaz Reza and Saha, Shuvra and Chowdhury, Mohammad Jamilur Reza and Deowan, Md. Ether},
  booktitle={2023 3rd International Conference on Robotics, Electrical and Signal Processing Techniques (ICREST)}, 
  title={A Simulation of a Robot Operating System Based Autonomous Wheelchair with Web Based HMI Using Rosbridge}, 
  year={2023},
  volume={},
  number={},
  pages={175-180},
  keywords={Performance evaluation;Costs;Navigation;Wheelchairs;Simulation;Operating systems;Signal processing;Robot Operating System;Gazebo;Autonomous Navigation;Cloud Control;rosbridge},
  doi={10.1109/ICREST57604.2023.10070046}
}

@article{Wilkinson,
url = {https://doi.org/10.1515/pjbr-2021-0023},
title = {Design guidelines for human–robot interaction with assistive robot manipulation systems},
title = {},
author = {Alexander Wilkinson and Michael Gonzales and Patrick Hoey and David Kontak and Dian Wang and Noah Torname and Amelia Sinclaire and Zhao Han and Jordan Allspaw and Robert Platt and Holly Yanco},
pages = {392--401},
volume = {12},
number = {1},
journal = {Paladyn},
doi = {doi:10.1515/pjbr-2021-0023},
year = {2021},
lastchecked = {2024-08-12}
}

@ARTICLE{9761203,
  author={Zhou, Longfei and Zhang, Lin and Konz, Nicholas},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems}, 
  title={Computer Vision Techniques in Manufacturing}, 
  year={2023},
  volume={53},
  number={1},
  pages={105-117},
  keywords={Image edge detection;Image segmentation;Task analysis;Robot sensing systems;Sensors;Feature detection;Three-dimensional displays;Assembly;computer vision (CV);deep learning;inspection;machine intelligence;machine learning;manufacturing;production;robotics;survey},
  doi={10.1109/TSMC.2022.3166397}
}

@article{BREZANI2022298,
title = {Smart extensions to regular cameras in the industrial environment},
journal = {Procedia Computer Science},
volume = {200},
pages = {298-307},
year = {2022},
note = {3rd International Conference on Industry 4.0 and Smart Manufacturing},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2022.01.228},
url = {https://www.sciencedirect.com/science/article/pii/S187705092200237X},
author = {S. Brezani and R. Hrasko and P. Vojtas},
keywords = {Unsupervised learning, performance improvement of industrial processes, object recognition, pseudo-ground truth, AI, machine learning in Industry},
abstract = {Data mining from unstructured data can be skillfully employed to improve the performance of manufacturing or industrial processes. The main goal of this paper is to create a fast emergency aid system for object detection in SME industrial premises. The basic assumption is that SMEs do not have any IT-trained personnel, and the solution has to be unsupervised edge computing. We use several off-the-shelf models of deep neural networks pre-trained for smart city applications, ready for online object recognition and edge computing. Our system works without any retraining, additional annotation, or human intervention. Specifically, we present heuristics for the automated creation of PGT (Pseudo-Ground Truth). Based on PGT, we can automatically decide which model is the best in the specific environment. We present an application of fully automated enhancing image capture camera outputs to smarter ones. We evaluate our system in a controlled experiment. Low-resolution cameras and large areas cause problems for our method. We present a proof-of-concept for improving our system even in these challenging situations. The benefit is a knowledge extraction in a simple and inexpensive way to expand the organizations’ databases with information from unstructured data from CCTV/IP cameras.}
}

@article{BARNES2010339,
title = {Visual detection of blemishes in potatoes using minimalist boosted classifiers},
journal = {Journal of Food Engineering},
volume = {98},
number = {3},
pages = {339-346},
year = {2010},
issn = {0260-8774},
doi = {https://doi.org/10.1016/j.jfoodeng.2010.01.010},
url = {https://www.sciencedirect.com/science/article/pii/S0260877410000142},
author = {Michael Barnes and Tom Duckett and Grzegorz Cielniak and Graeme Stroud and Glyn Harper},
keywords = {AdaBoost, Machine learning, Potatoes, Visual inspection of produce, Blemish detection},
abstract = {This paper introduces novel methods for detecting blemishes in potatoes using machine vision. After segmentation of the potato from the background, a pixel-wise classifier is trained to detect blemishes using features extracted from the image. A very large set of candidate features, based on statistical information relating to the colour and texture of the region surrounding a given pixel, is first extracted. Then an adaptive boosting algorithm (AdaBoost) is used to automatically select the best features for discriminating between blemishes and non-blemishes. With this approach, different features can be selected for different potato varieties, while also handling the natural variation in fresh produce due to different seasons, lighting conditions, etc. The results show that the method is able to build “minimalist” classifiers that optimise detection performance at low computational cost. In experiments, blemish detectors were trained for both white and red potato varieties, achieving 89.6% and 89.5% accuracy, respectively.}
}

@article{THROOP2005281,
title = {Quality evaluation of apples based on surface defects: development of an automated inspection system},
journal = {Postharvest Biology and Technology},
volume = {36},
number = {3},
pages = {281-290},
year = {2005},
issn = {0925-5214},
doi = {https://doi.org/10.1016/j.postharvbio.2005.01.004},
url = {https://www.sciencedirect.com/science/article/pii/S0925521405000177},
author = {J.A. Throop and D.J. Aneshansley and W.C. Anger and D.L. Peterson},
keywords = {Apples, Apple grading, Automated inspection, Apple sorting, Defects, Defect detection},
abstract = {The development of an automated inspection station to grade processing apples includes a conveyor for apple orientation, optics and camera to capture identical images at three predetermined wavebands, a lighting system that illuminates the apple's surface diffusely and image processing algorithms to segment surface defects on apples in real time. The conveyor oriented apples so that the stem/calyx ends were not visible during image capture. Multi-spectral optics fabricated using a multi-vision linear filter mounted in front of the camera lens provided three different waveband (740, 950nm and visible) images of apples on a single camera array. Interference filters placed in the optical path provided the different wavebands. The diameter and height of each apple was measured to estimate the apple's volume. These dimensions and the position of the apple in the image allowed a portion of each image to be defined, the so-called region of interest (ROI). These sub-images made a composite image of the apple's surface.}
}

@article{BURGOSARTIZZU2010138,
title = {Analysis of natural images processing for the extraction of agricultural elements},
journal = {Image and Vision Computing},
volume = {28},
number = {1},
pages = {138-149},
year = {2010},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2009.05.009},
url = {https://www.sciencedirect.com/science/article/pii/S0262885609001140},
author = {Xavier P. Burgos-Artizzu and Angela Ribeiro and Alberto Tellaeche and Gonzalo Pajares and Cesar Fernández-Quintanilla},
keywords = {Computer vision, Precision agriculture, Weed detection, Parameter setting, Genetic algorithms},
abstract = {This work presents several developed computer-vision-based methods for the estimation of percentages of weed, crop and soil present in an image showing a region of interest of the crop field. The visual detection of weed, crop and soil is an arduous task due to physical similarities between weeds and crop and to the natural and therefore complex environments (with non-controlled illumination) encountered. The image processing was divided in three different stages at which each different agricultural element is extracted: (1) segmentation of vegetation against non-vegetation (soil), (2) crop row elimination (crop) and (3) weed extraction (weed). For each stage, different and interchangeable methods are proposed, each one using a series of input parameters which value can be changed for further refining the processing. A genetic algorithm was then used to find the best value of parameters and method combination for different sets of images. The whole system was tested on several images from different years and fields, resulting in an average correlation coefficient with real data (bio-mass) of 84%, with up to 96% correlation using the best methods on winter cereal images and of up to 84% on maize images. Moreover, the method’s low computational complexity leads to the possibility, as future work, of adapting them to real-time processing.}
}

@article{PANERU2021103940,
title = {Computer vision applications in construction: Current state, opportunities and challenges},
journal = {Automation in Construction},
volume = {132},
pages = {103940},
year = {2021},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2021.103940},
url = {https://www.sciencedirect.com/science/article/pii/S0926580521003915},
author = {Suman Paneru and Idris Jeelani},
keywords = {Computer vision, Technology in construction, Visual data analytics, Monitoring},
abstract = {Thousands of images and videos are collected from construction projects during construction. These contain valuable data that, if harnessed efficiently, can help automate or at least reduce human effort in diverse construction management activities such as progress monitoring, safety management, quality control and productivity tracking. Extracting meaningful information from images requires the development of technology and algorithms that enable computers to understand digital images or videos, replicating the functionality of human visual systems. This is the goal of computer vision. This review aims at providing an updated and categorized overview of computer vision applications in construction by examining the recent developments in the field and identifying the opportunities and challenges that future research needs to address to fully leverage the potential benefits of Computer Vision. We restrict the focus to four areas that can benefit the most from computer vision - Safety Management, Progress Monitoring, Productivity Tracking and Quality Control.}
}

@INPROCEEDINGS{7892717,
  author={Krishna and Poddar, Madhav and Giridhar M K and Prabhu, Amit Suresh and Umadevi V},
  booktitle={2016 International Conference on ICT in Business Industry and Government (ICTBIG)}, 
  title={Automated traffic monitoring system using computer vision}, 
  year={2016},
  volume={},
  number={},
  pages={1-5},
  keywords={Surveillance;Cameras;Traffic control;Computer vision;Lighting;Estimation;multiple reference lines;vehicle count;pixel values variation;traffic analysis;speed estimation;speed violation identification},
  doi={10.1109/ICTBIG.2016.7892717}
}

@article{COIFMAN1998271,
title = {A real-time computer vision system for vehicle tracking and traffic surveillance},
journal = {Transportation Research Part C: Emerging Technologies},
volume = {6},
number = {4},
pages = {271-288},
year = {1998},
issn = {0968-090X},
doi = {https://doi.org/10.1016/S0968-090X(98)00019-9},
url = {https://www.sciencedirect.com/science/article/pii/S0968090X98000199},
author = {Benjamin Coifman and David Beymer and Philip McLauchlan and Jitendra Malik},
keywords = {Traffic surveillance, Wide-area detection, Vehicle tracking, Video image processing, Machine vision},
abstract = {Increasing congestion on freeways and problems associated with existing detectors have spawned an interest in new vehicle detection technologies such as video image processing. Existing commercial image processing systems work well in free-flowing traffic, but the systems have difficulties with congestion, shadows and lighting transitions. These problems stem from vehicles partially occluding one another and the fact that vehicles appear differently under various lighting conditions. We are developing a feature-based tracking system for detecting vehicles under these challenging conditions. Instead of tracking entire vehicles, vehicle features are tracked to make the system robust to partial occlusion. The system is fully functional under changing lighting conditions because the most salient features at the given moment are tracked. After the features exit the tracking region, they are grouped into discrete vehicles using a common motion constraint. The groups represent individual vehicle trajectories which can be used to measure traditional traffic parameters as well as new metrics suitable for improved automated surveillance. This paper describes the issues associated with feature based tracking, presents the real-time implementation of a prototype system, and the performance of the system on a large data set. ©}
}

@book{jordan2016robots,
  title={Robots},
  author={Jordan, John M},
  year={2016},
  publisher={Mit Press}
}

% ROS --start
@online{rosblog,
  author = {ROS community},
  title = {Why ROS? It's the fastest way to build a robot!},
  year = {2024},
  url = {https://www.ros.org/blog/why-ros/},
  note = {Accessed: 2024-07-12}
}

@book{koubaa2017robot,
  title={Robot Operating System (ROS).},
  author={Koubaa, Anis and others},
  volume={1},
  year={2017},
  publisher={Springer}
}

@inproceedings{takaya2016simulation,
  title={Simulation environment for mobile robots testing using ROS and Gazebo},
  author={Takaya, Kenta and Asai, Toshinori and Kroumov, Valeri and Smarandache, Florentin},
  booktitle={2016 20th International Conference on System Theory, Control and Computing (ICSTCC)},
  pages={96--101},
  year={2016},
  organization={IEEE}
}

@inproceedings{qian2014manipulation,
  title={Manipulation task simulation using ROS and Gazebo},
  author={Qian, Wei and Xia, Zeyang and Xiong, Jing and Gan, Yangzhou and Guo, Yangchao and Weng, Shaokui and Deng, Hao and Hu, Ying and Zhang, Jianwei},
  booktitle={2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)},
  pages={2594--2598},
  year={2014},
  organization={IEEE}
}


% ROS --end


@incollection{HAVLIK2011327,
title = {14 - Robotic tools for de-mining and risky operations},
editor = {Y. Baudoin and Maki K. Habib},
booktitle = {Using Robots in Hazardous Environments},
publisher = {Woodhead Publishing},
pages = {327-352},
year = {2011},
isbn = {978-1-84569-786-0},
doi = {https://doi.org/10.1533/9780857090201.3.327},
url = {https://www.sciencedirect.com/science/article/pii/B9781845697860500149},
author = {Š. Havlík},
keywords = {de-mining vehicle, robotic approach, modular concept, mine detection, remote control},
abstract = {Abstract:
The chapter deals with applications of mobile robotic technology for performing risky tasks, mainly de-mining operations. The robotic approach and problems related to the de-mining process are analyzed in more detail and some specific performance features that an unmanned robotic agent should exhibit are discussed. The modular concept of the remotely controlled robotic vehicle, which consists of the general mobility system and a set of exchangeable task oriented tools, is also discussed. As an illustrative example, the brief history and current state of development of the remotely controlled vehicle ‘Božena’, with flailing activation mechanism and other available tools, are presented.}
}

@INPROCEEDINGS{10201199,
  author={Sahan, A.S. Mohamed and Kathiravan, S. and Lokesh, M. and Raffik, R.},
  booktitle={2023 2nd International Conference on Advancements in Electrical, Electronics, Communication, Computing and Automation (ICAECA)}, 
  title={Role of Cobots over Industrial Robots in Industry 5.0: A Review}, 
  year={2023},
  volume={},
  number={},
  pages={1-5},
  keywords={Industries;Productivity;Automation;Service robots;Industrial robots;Turning;Safety;Industry 4.0;industrial robots;human-machine collaboration;Industry 5.0;collaborative robots},
  doi={10.1109/ICAECA56562.2023.10201199}
}
@article{Wakizako,
title = {Industrial Robots and Machine Vision},
journal = {	The 3rd International Conference on Industrial Application Engineering 2015 (ICIAE2015)},
year = {2015},
doi = {10.12792/iciae2015.003},
author = {Hitoshi Wakizako},
abstract = {Industrial robots are used for various applications such as welding, handling, painting, inspection, and assembly in
automated production lines, especially in car manufacturing production. The typical operation of industrial robots is called
“teaching and playback”. In the teaching operation, an operator moves a robot to the series of points located on desired path
of the robot using a teach pendant and stores those positions. In the playback operation, the robot moves to the stored
positions sequentially and repeats the sequence. Because the robot follow the same path consist of the programed positions,
the target parts of the robot are required to be pre-oriented and aligned using part aligners or fixtures before the robot starts its
task.
Machine vision systems allow a robot to perform in more flexible manner. The vision systems are used for such
applications as detection of part orientation or position, part inspection and measurement. In the vision system, cameras
acquire object images, and the images are transferred to computers and analyzed using image processing software.
The result of the image processing is transferred to a robot controller and the position data of the robot is modified.
In this keynote speech, the roles of industrial robots and machine vision are presented.}
}

@article{SathishKumar2023,
author={Sathish Kumar, A.
and Naveen, S.
and Vijayakumar, R.
and Suresh, V.
and Asary, Abdul Rab
and Madhu, S.
and Palani, Kumaran},
title={An intelligent fuzzy-particle swarm optimization supervisory-based control of robot manipulator for industrial welding applications},
journal={Scientific Reports},
year={2023},
month={May},
day={22},
volume={13},
number={1},
pages={8253},
abstract={The propensity of manufacturers to produce goods at affordable cost, with more accuracy, and at a faster rate force them to search for novel solutions, such as deploying robots in place of people in a sector that can accommodate their needs. Welding is one of the most crucial processes in the automotive industry. This process is time-consuming, subject to error, and demands skilled professionals. The robotic application can improve this area of production and quality. Other industries, such as painting and material handling, can also profit from the use of robots. This work describes the fuzzy DC linear servo controller, which functions as a robotic arm actuator. Robots have been widely employed in most productive sectors in recent years, including assembly plates, welding, tasks at higher temperatures, etc. Controlling a robot accurately is a difficult undertaking as a robot is very nonlinear with many joints that are often organized and unstructured. To carry out the effective task, an effective PID control based on fuzzy logic has been employed together with the method of Particle Swarm Optimization (PSO) approach for the estimate of the parameter. This offline technique determines the lowest number of optimal robotic arm control parameters. To verify the controller design with computer simulation, a comparative assessment of controllers is given by means of a fuzzy surveillance controller with PSO which improves the parameter gain to provide a rapid climb, a smaller overflow, no steady condition error signal, and effective torque control of the robot arm.},
issn={2045-2322},
doi={10.1038/s41598-023-35189-2},
url={https://doi.org/10.1038/s41598-023-35189-2}
}

% 2.2 Automated bending process references
@article{alvaautomated,
  title={AUTOMATED DESIGN OF SHEET METAL BENDING TOOLS},
  author={Alva, Ujval and Gupta, Satyandra K},
  publisher={Citeseer}
}

@online{bmspecifications,
  author = {AMADA},
  title = {Bending Technology HFP Specifications},
  year = {2024},
  url = {https://m.amada.de/en/bending/hfp/specifications.html},
  note = {Accessed: 2024-08-21}
}

@online{astro100,
  author = {AMADA},
  title = {In zweiter Generation – die Roboter-Biegezellen der Serie ASTRO-100 II NT},
  year = {2024},
  url = {https://m.amada.de/de/abkanten/astro-100-ii/biegeroboter.html},
  note = {Accessed: 2024-08-21}
}

% AMADA bending machine --start
@online{amada-machine,
  author = {AMADA},
  title = {HFP Description},
  year = {2005},
  url = {https://www.amada.de/en/bending/hfp/description.html},
  note = {Accessed: 2024-08-05}
}

@misc{liu2022metalwiremanipulationplanning,
      title={Metal Wire Manipulation Planning for 3D Curving -- How a Low Payload Robot Can Use a Bending Machine to Bend Stiff Metal Wire}, 
      author={Ruishuang Liu and Weiwei Wan and Emiko Isomura and Kensuke Harada},
      year={2022},
      eprint={2203.04024},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2203.04024}, 
}

@online{shenchong,
  author = {Shenchong},
  title = {Automated Bending Cell- Robot Press Brake},
  year = {2024},
  url = {https://www.shenchong.com/robotic-press-brake.html},
  note = {Accessed: 2024-07-30}
}

@online{mekoprint,
  author = {Mekoprint Mechanics},
  title = { Robotic bending solution - Mekoprint Mechanics},
  year = {2024},
  url = {https://www.youtube.com/watch?v=EhQ7DvXjDCA},
  note = {Accessed: 2024-07-30}
}

@online{shopmetal,
  author = {Shop Metalworking Techology},
  title = {Robotic bending cell},
  year = {2023},
  url = {https://www.youtube.com/watch?v=EhQ7DvXjDCA},
  note = {Accessed: 2024-08-04}
}

@inproceedings{guimaraes2009bending,
  title={A bending cell for small batches},
  author={Guimar{\~a}es, Rui J and Pacheco, Jos{\'e} A and Meireles, Jos{\'e} F and Fonseca, Jaime F},
  booktitle={7th euromech solid mechanics conference},
  year={2009}
}


@online{cobotfabricator,
  author = {Dan Davis},
  title = {Metal fabricator finds flexibility in its bending department with cobots},
  year = {2023},
  url = {https://www.youtube.com/watch?v=EhQ7DvXjDCA},
  note = {Accessed: 2024-07-25}
}

% 2.1 Robotic automation in manufacturing
@ARTICLE{10381692,
  author={Gómez-Hernández, José-Francisco and Gutiérrez-Hernández, José-María and Jimeno-Morenilla, Antonio and Sánchez-Romero, José-Luis and Fabregat-Periago, María-Dolores},
  journal={IEEE Access}, 
  title={Development of an Integrated Robotic Workcell for Automated Bonding in Footwear Manufacturing}, 
  year={2024},
  volume={12},
  number={},
  pages={5066-5080},
  abstract={Traditional manufacturing industries are currently immersed in an automation process, integrating new techniques and tools, driven by the demands from producers to improve the manufacturing process as well as the working conditions of employees. For the footwear industry, bonding is a key operation in the manufacturing process where the outsole is assembled onto the lasted shoe. However, in this operation, workers are often subjected to hazardous substances (i.e., organic solvents) and perform repetitive tasks with limited added value. Against this background, this paper describes the results of a research project, whose aim was to obtain the maximum benefit from different technologies analyzed, such as collaborative robotics, artificial vision and multirobot control, for the manipulation of flexible/deformable objects. The main result of this project is a robotic workcell for shoe bonding that has been introduced in the production line to fully automate the operation. This workcell integrates three collaborative robots, one for (hot melt) adhesive application and another two, with two-finger electric grippers, to carry out the bonding synchronously. Different vision systems have also been embedded to conduct the various processes involved. The entire operation is controlled and coordinated through ROS (Robot Operating System). The key findings of this research showcase the automation of a process traditionally undertaken by humans. In this novel approach, two robots collaborate to manipulate flexible objects, liberating the operator from engaging in repetitive, non-value-added tasks and the handling of hazardous substances.},
  keywords={Robots;Europe;Service robots;Planning;Manufacturing;Collision avoidance;Bonding;Fourth Industrial Revolution;Robotics and automation;Smart manufacturing;Bonding;footwear;fourth industrial revolution;robotics and automation;smart manufacturing;manufacturing automation},
  doi={10.1109/ACCESS.2024.3350441},
  ISSN={2169-3536},
  month={},
}

@online{firstrobot,
  author = {Infosys BPM},
  title = {Rise of the machines: Robotics’ impact on the evolution of manufacturing},
  year = {2024},
  url = {https://www.infosysbpm.com/blogs/manufacturing/robotics-in-manufacturing.html},
  note = {Accessed: 2024-07-25}
}


@online{jrautomation,
  author = {JR Automation - A Hitachi Group Company},
  title = {How the Manufacturing Industry uses Robotics \& Automation},
  year = {2024},
  url = {https://www.jrautomation.com/resources/manufacturing-robotics-automation},
  note = {Accessed: 2024-07-25}
}

@online{jrautomation2,
  author = {JR Automation - A Hitachi Group Company},
  title = {The Future of Manufacturing Automation},
  year = {2024},
  url = {https://www.jrautomation.com/resources/the-future-of-manufacturing-automation},
  note = {Accessed: 2024-07-25}
}

@article{li2020robotics,
  title={Robotics in manufacturing—The past and the present},
  author={Li, Ming and Milojevi{\'c}, Andrija and Handroos, Heikki},
  journal={Technical, Economic and Societal Effects of Manufacturing 4.0: Automation, Adaption and Manufacturing in Finland and Beyond},
  pages={85--95},
  year={2020},
  publisher={Springer International Publishing}
}

@book{tarn2011robotic,
  title={Robotic welding, intelligence and automation: RWIA’2010},
  author={Tarn, Tzyh-Jong and Chen, Shan-Ben and Fang, Gu},
  volume={88},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{SKIBNIEWSKI1992251,
title = {Robotic materials handling for automated building construction technology},
journal = {Automation in Construction},
volume = {1},
number = {3},
pages = {251-266},
year = {1992},
issn = {0926-5805},
doi = {https://doi.org/10.1016/0926-5805(92)90017-E},
url = {https://www.sciencedirect.com/science/article/pii/092658059290017E},
author = {Mirosław J. Skibniewski and Stephen C. Wooldridge},
keywords = {Automated building construction systems, automated material handling systems, robotic material handling, bar code technology},
abstract = {Several leading Japanese construction firms are developing fully automated, self-rising platforms for the construction of high-rise buildings. These automated building construction systems provide an integrated building construction environment for robotized cranes, finishing robots, computer work stations, and other automated construction equipment. A number of benefits are anticipated from these systems, including improved construction productivity, less dependence on labor, and improved safety and quality. The impact of this integrated automation approach is expected to be significant due to its high level of coordination between resources and processes, and well defined environment for information transfer. As a follow-up to this effort, several research issues need to be considered, including the design of materials handling systems which will maintain the efficiency of the automated building construction approach. This paper describes an automated materials handling system concept for managing and handling construction materials within automated building construction systems. The materials handling system is based on proven automation technologies and a distributed computer network. A prototype robotic materials handling workcell based on a micro-robot and bar code technology, and developed for integration within the automated materials handling system is also described.}
}

@inproceedings{gambao2012new,
  title={A new generation of collaborative robots for material handling},
  author={Gambao, Ernesto and Hernando, Miguel and Surdilovic, Dragoljub},
  booktitle={ISARC. Proceedings of the International Symposium on Automation and Robotics in Construction},
  volume={29},
  pages={1},
  year={2012},
  organization={IAARC Publications}
}

@article{ji2021learning,
  title={Learning-based automation of robotic assembly for smart manufacturing},
  author={Ji, Sanghoon and Lee, Sukhan and Yoo, Sujeong and Suh, Ilhong and Kwon, Inso and Park, Frank C and Lee, Sanghyoung and Kim, Hongseok},
  journal={Proceedings of the IEEE},
  volume={109},
  number={4},
  pages={423--440},
  year={2021},
  publisher={IEEE}
}

@inproceedings{shah2021design,
  title={Design of gripper and selection of robotic arm for automation of a pick and place process},
  author={Shah, Vijesh and Gilke, Nandkumar and Dhore, Vilas and Phutane, Chandrashekhar and Kondhol, Bhavisha},
  booktitle={Advances in Manufacturing Systems: Select Proceedings of RAM 2020},
  pages={95--107},
  year={2021},
  organization={Springer}
}

@article{lee2021intelligent,
  title={Intelligent robotic palletizer system},
  author={Lee, Jeng-Dao and Chang, Chen-Huan and Cheng, En-Shuo and Kuo, Chia-Chen and Hsieh, Chia-Ying},
  journal={Applied Sciences},
  volume={11},
  number={24},
  pages={12159},
  year={2021},
  publisher={MDPI}
}

@Inbook{Uhrhan1995,
author="Uhrhan, Christoph
and Roshardt, Ren{\'e}
and Schweitzer, Gerhard",
editor="L{\"u}ckel, Joachim",
title="User Oriented Automation of Flexible Sheet Bending",
bookTitle="Proceedings of the Third Conference on Mechatronics and Robotics: ``From design methods to industrial applications''",
year="1995",
publisher="Vieweg+Teubner Verlag",
address="Wiesbaden",
pages="202--211",
abstract="The automation concept is based on the integration of the three factors ``man, technique, organisation --- MTO'', developed at the Centre for Integrated Production at the ETH. As a benchmark, a manufacturing cell for flexible bending of metal sheets is under construction. It consists of a laser cutting machine, press brake, conveyor belts, and a robot. These devices will be networked to enable the fully automated programming cycle. The project is jointly pursued by the Institute of Work Psychology, the Institute of Forming Technology, and the Robotics Lab.",
isbn="978-3-322-91170-4",
doi="10.1007/978-3-322-91170-4_15",
url="https://doi.org/10.1007/978-3-322-91170-4_15"
}

% requirements

% camera recalibration
@article{Bahadir2024,
author={Bahadir, Ozan
and Siebert, Jan Paul
and Aragon-Camarasa, Gerardo},
title={Continual learning approaches to hand--eye calibration in robots},
journal={Machine Vision and Applications},
year={2024},
month={Jul},
day={10},
volume={35},
number={4},
pages={97},
abstract={This study addresses the problem of hand--eye calibration in robotic systems by developing Continual Learning (CL)-based approaches. Traditionally, robots require explicit models to transfer knowledge from camera observations to their hands or base. However, this poses limitations, as the hand--eye calibration parameters are typically valid only for the current camera configuration. We, therefore, propose a flexible and autonomous hand--eye calibration system that can adapt to changes in camera pose over time. Three CL-based approaches are introduced: the naive CL approach, the reservoir rehearsal approach, and the hybrid approach combining reservoir sampling with new data evaluation. The naive CL approach suffers from catastrophic forgetting, while the reservoir rehearsal approach mitigates this issue by sampling uniformly from past data. The hybrid approach further enhances performance by incorporating reservoir sampling and assessing new data for novelty. Experiments conducted in simulated and real-world environments demonstrate that the CL-based approaches, except for the naive approach, achieve competitive performance compared to traditional batch learning-based methods. This suggests that treating hand--eye calibration as a time sequence problem enables the extension of the learned space without complete retraining. The adaptability of the CL-based approaches facilitates accommodating changes in camera pose, leading to an improved hand--eye calibration system.},
issn={1432-1769},
doi={10.1007/s00138-024-01572-w},
url={https://doi.org/10.1007/s00138-024-01572-w}
}

% Overview of the workcell --start
@online{urdf,
  author = {ROS Wiki},
  title = {urdf},
  year = {2024},
  url = {http://wiki.ros.org/urdf},
  note = {Accessed: 2024-07-29}
}

% Overview --end

% Kassow robots --start

@online{kassow-specification,
  author = {Kassow Robots},
  title = {Technical Specifications},
  year = {2024},
  url = {https://www.kassowrobots.com/products/7-axis-collaborative-robot-arm-kr-series},
  note = {Accessed: 2024-08-05}
}

@manual{kassow-manual,
  author = {Kassow Robots},
  title = {Product Manual},
  volume={Generation 2, 4.0},
  year={2024},
  url = {https://www.kassowrobots.com/downloads/product-manuals},
}

@manual{kassow-software-manual,
  author = {Kassow Robots},
  title = {Software Manual PDF},
  volume={1.3.0},
  year={2024},
  url = {https://docs.kassowrobots.com/kr-assets/manuals/software-manual-kassowrobots_v1.3.0p.pdf},
}

@manual{profinet-manual,
  author = {Kassow Robots},
  title = {Software Manual PDF},
  volume={Version: 1.0.0 EN1},
  year={2024},
  url = {https://docs.kassowrobots.com/profinet-kassow_robots_v1.0.1.pdf},
}

@online{Cbun,
  author = {Kassow Robots},
  title = {CBuns Development},
  year = {2024},
  url = {https://docs.kassowrobots.com/en/cbuns/development},
  note = {Accessed: 2024-08-06}
}

@online{profinet,
  author = {Kassow Robots},
  title = {Profinet Support},
  year = {2024},
  url = {https://docs.kassowrobots.com/en/resources/Profinet},
  note = {Accessed: 2024-08-06}
}


@online{kassow-ros,
  author = {Kassow Robots},
  title = {ROS Interface},
  year = {2024},
  url = {https://www.kassowrobots.com/ecosystem/kr-pulse/manufacturers/kassow-robots},
  note = {Accessed: 2024-08-06}
}
% Kassow robots --end


% sensopart --start
@online{sensopart-visor,
  author = {SENSOPART},
  title = {Hardware + Software = VISOR®},
  year = {2024},
  url = {https://www.sensopart.com/en/products/vision-sensors/},
  note = {Accessed: 2024-08-09}
}

@misc{ISO13849,
  title        = {{EN ISO 13849-1:2015 Safety of Machinery - Safety-related Parts of Control Systems}},
  year         = {2015},
  url          = {https://www.iso.org/standard/69883.html},
  organization = {International Organization for Standardization},
  note         = {Standard No. ISO 13849-1:2015}
}

@online{sensopart-software,
  author = {SENSOPART},
  title = {VISOR® PC software},
  year = {2024},
  url = {https://www.sensopart.com/en/service/downloads/90-visor-pc-software/},
  note = {Accessed: 2024-08-09}
}

@online{visor-robotic,
  author = {SENSOPART},
  title = {VISOR® V20 Robotic Advanced, wide field of view},
  year = {2024},
  url = {https://www.sensopart.com/en/products/details/632-91067/},
  note = {Accessed: 2024-08-09}
}

@online{visor-object,
  author = {SENSOPART},
  title = {VISOR® V20 Object Advanced, wide field of view},
  year = {2024},
  url = {https://www.sensopart.com/en/products/details/632-91034/},
  note = {Accessed: 2024-08-09}
}

% PLC --start
@book{bolton2015programmable,
  title={Programmable logic controllers},
  author={Bolton, William},
  year={2015},
  publisher={Newnes}
}

@article{ALPHONSUS20161185,
title = {A review on the applications of programmable logic controllers (PLCs)},
journal = {Renewable and Sustainable Energy Reviews},
volume = {60},
pages = {1185-1205},
year = {2016},
issn = {1364-0321},
doi = {https://doi.org/10.1016/j.rser.2016.01.025},
url = {https://www.sciencedirect.com/science/article/pii/S1364032116000551},
author = {Ephrem Ryan Alphonsus and Mohammad Omar Abdullah},
keywords = {PLC, Energy systems, Industrial control, Monitoring},
abstract = {As the need of automation increases significantly, a control system needs to be easily programmable, flexible, reliable, robust and cost effective. In this paper a review on the application of programmable logic controller (PLC) in our current market is discussed. Investigations on the applications of PLCs in energy research, engineering studies, industrial control applications and monitoring of plants are reviewed in this paper. PLCs do have its own limitations, but findings indicate that PLCs have more advantages than limitations. This paper concludes that PLCs can be used for any applications whether it is of simple or complicated control system.}
}

@online{siemens,
  author = {SIEMENS},
  title = {SIMATIC controllers – passion for automation},
  year = {2024},
  url = {https://www.siemens.com/global/en/products/automation/systems/industrial/plc.html},
  note = {Accessed: 2024-08-10}
}

@online{schunk-gripper,
  author = {Schunk},
  title = {PGN-plus-P, Universal gripper},
  year = {2024},
  url = {https://schunk.com/de/en/gripping-systems/parallel-gripper/pgn-plus-p/c/PGR_3228},
  note = {Accessed: 2024-08-10}
}

% Software architecture --start
@online{gazebo-classic,
  author = {Gazebo},
  title = {Beginner: Overview. What is Gazebo?},
  year = {2014},
  url = {https://classic.gazebosim.org/tutorials?cat=guided_b&tut=guided_b1},
  note = {Accessed: 2024-08-11}
}

@online{rosnode,
  author = {ROS.org},
  title = {Understanding ROS Nodes},
  year = {2022},
  url = {http://wiki.ros.org/ROS/Tutorials/UnderstandingNodes},
  note = {Accessed: 2024-08-11}
}

@online{rostopic,
  author = {ROS.org},
  title = {Understanding ROS Topics},
  year = {2022},
  url = {http://wiki.ros.org/ROS/Tutorials/UnderstandingTopics},
  note = {Accessed: 2024-08-11}
}

@online{rospackage,
  author = {ROS.org},
  title = {Creating a ROS Package},
  year = {2022},
  url = {http://wiki.ros.org/ROS/Tutorials/CreatingPackage},
  note = {Accessed: 2024-08-11}
}


@online{roslaunch,
  author = {ROS.org},
  title = {roslaunch},
  year = {2019},
  url = {http://wiki.ros.org/roslaunch},
  note = {Accessed: 2024-08-11}
}


@online{rosservice,
  author = {ROS.org},
  title = { rosservice },
  year = {2011},
  url = {http://wiki.ros.org/rosservice},
  note = {Accessed: 2024-08-11}
}

@online{parameterserver,
  author = {ROS.org},
  title = {Parameter Server},
  year = {2018},
  url = {http://wiki.ros.org/Parameter%20Server},
  note = {Accessed: 2024-08-11}
}


@online{actionserver,
  author = {ROS.org},
  title = {Writing a Simple Action Server using the Execute Callback},
  year = {2018},
  url = {http://wiki.ros.org/actionlib_tutorials/Tutorials/SimpleActionServer%28ExecuteCallbackMethod%29},
  note = {Accessed: 2024-08-11}
}

@online{moveit,
  author = {MoveIt},
  title = {Moving robots into the future},
  year = {2024},
  url = {https://moveit.ai/},
  note = {Accessed: 2024-08-11}
}

@online{rviz,
  author = {MoveIt},
  title = {rviz - User Guide},
  year = {2020},
  url = {http://wiki.ros.org/rviz/UserGuide},
  note = {Accessed: 2024-08-11}
}

@online{ros3djs,
  author = {ROS Wiki},
  title = {3D Visualization Library for use with the ROS JavaScript Libraries},
  year = {2015},
  url = {https://wiki.ros.org/ros3djs},
  note = {Accessed: 2024-08-12}
}

@online{reactjs,
  author = {React},
  title = {React: The library for web and native user interfaces},
  year = {2024},
  url = {https://react.dev/},
  note = {Accessed: 2024-08-12}
}

@online{rosbridge,
  author = {ROS Wiki},
  title = {rosbridge-suite},
  year = {2022},
  url = {http://wiki.ros.org/rosbridge_suite},
  note = {Accessed: 2024-08-12}
}

@online{roslib,
  author = {ROS Wiki},
  title = {roslib: Package Summary},
  year = {2013},
  url = {http://wiki.ros.org/roslib},
  note = {Accessed: 2024-08-12}
}

@online{webtools,
  author = {Robot Web tools},
  title = {Robot Web Tools},
  year = {2024},
  url = {https://robotwebtools.github.io/},
  note = {Accessed: 2024-08-12}
}

@online{websocket,
  author = {Robot Web tools},
  title = {The WebSocket API (WebSockets)},
  year = {2024},
  url = {https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API},
  note = {Accessed: 2024-08-12}
}


@online{threejs,
  author = {three.js},
  title = {Fundamentals},
  year = {2024},
  url = {https://threejs.org/manual/#en/fundamentals},
  note = {Accessed: 2024-08-12}
}

@online{visualization-rwt,
  author = {tork-a},
  title = {visualization-rwt},
  year = {2024},
  url = {https://github.com/tork-a/visualization_rwt},
  note = {Accessed: 2024-08-12}
}
@online{frontend,
  author = {kamrify},
  title = {What is Frontend Development?},
  year = {2024},
  url = {https://roadmap.sh/frontend},
  note = {Accessed: 2024-08-12}
}

@online{backend,
  author = {kamrify},
  title = {What is Backend Development?},
  year = {2024},
  url = {https://roadmap.sh/backend},
  note = {Accessed: 2024-08-12}
}

@online{nodejs,
  author = {node.js},
  title = {About Node.js},
  year = {2024},
  url = {https://nodejs.org/en/about},
  note = {Accessed: 2024-08-12}
}

@online{npm,
  author = {npm},
  title = {About Node.js},
  year = {2024},
  url = {https://www.npmjs.com/},
  note = {Accessed: 2024-08-12}
}

























